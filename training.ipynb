{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283 documents\n",
      "129 classes ['What are the types of depression?', 'about', 'afternoon', 'anxious', 'ask', 'at what age does anxiety peak?', 'can lack of sleep make you feel sad?', 'can low blood sugar cause suicidal thoughts?', 'casual', 'creation', 'death', 'default', 'depressed', 'do we control our thoughts?', 'does oversleeping cause depression?', 'done', 'evening', 'fact-1', 'fact-10', 'fact-11', 'fact-12', 'fact-13', 'fact-14', 'fact-15', 'fact-16', 'fact-17', 'fact-18', 'fact-19', 'fact-2', 'fact-20', 'fact-21', 'fact-22', 'fact-23', 'fact-24', 'fact-25', 'fact-26', 'fact-27', 'fact-28', 'fact-29', 'fact-3', 'fact-30', 'fact-31', 'fact-32', 'fact-5', 'fact-6', 'fact-7', 'fact-8', 'fact-9', 'friends', 'goodbye', 'greeting', 'happy', 'hate-me', 'hate-you', 'help', 'how can we reduce anxiety?', 'how does depression affect the world?', 'how long can anxiety last?', 'how many thoughts a day do we have?', 'i am a victim of bullying', 'i am afraid i will fail again', 'i am afraid to file a case against bullying', 'i am feeling anxious lately.', 'i am feeling stressed lately', 'i am good for nothing!', 'i am good for nothing.', 'i am lonely!', 'i am sad', 'i am stressed out', \"i can't do this anymore\", 'i feel i have let my parents down', 'i hate losing.', 'i hate myself!', 'i let everyojokne down', 'i think i am ugly!', \"i think i'm losing my mind\", 'i want a break', 'i want to kill myself', 'i want to leave the cou ntry and run away', 'i will never succeed in life', \"i wish i could've been a winner\", 'i wish i was better than them', 'i wish to quit', 'is depression a side effect of diabetes?', 'is school a cause of depression?', 'jamila-useful', 'jokes', 'learn-mental-health', 'learn-more', 'location', 'meditation', 'mental-health-fact', 'morning', 'my time has come', 'neutral-response', 'night', 'no one likes me!', 'no-approach', 'no-response', 'not-talking', 'problem', 'repeat', 'sad', 'scared', 'skill', 'sleep', 'something-else', 'stressed', 'stupid', 'suicide', 'thanks', 'understand', 'user-advice', 'user-agree', 'user-meditation', 'what are the causes of depression?', 'what are the stages of anxiety?', 'what are the top causes of depression?', 'what is depression?', 'what is the 3 3 3 rule for anxiety?', 'what is the biological cause of depression?', 'what is the meaning of anxiety and depression?', 'which age group has the highest rate of depression?', 'which country has the highest rate of depression?', 'which country has the lowest rate of depression?', 'which race has the highest rate of depression?', 'why is anxiety bad for you?', 'worthless', 'wrong']\n",
      "355 unique lemmatized words [\"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", ',', '.', '3', 'a', 'about', 'absolutely', 'advice', 'affect', 'afraid', 'afternoon', 'again', 'against', 'age', 'all', 'alot', 'already', 'am', 'and', 'another', 'answer', 'anxiety', 'anxious', 'any', 'anymore', 'anyone', 'anything', 'appears', 'approaching', 'are', 'ask', 'at', 'au', 'available', 'away', 'awful', 'bad', 'be', 'because', 'become', 'been', 'before', 'better', 'between', 'biological', 'blood', 'bonjour', 'boyfriend', 'break', 'bring', 'brother', 'bullying', 'burned', 'bye', 'ca', 'call', 'can', 'case', 'cause', 'cheerful', 'child', 'come', 'commit', 'connection', 'continue', 'control', 'cou', 'could', 'country', 'crazy', 'created', 'cure', 'dad', 'day', 'define', 'depressed', 'depression', 'deserve', 'diabetes', 'did', 'die', 'died', 'difference', 'different', 'disorder', 'do', 'doe', 'down', 'dumb', 'effect', 'else', 'empty', 'enough', 'evening', 'everyojokne', 'exam', 'fact', 'fail', 'family', 'fare', 'feel', 'feeling', 'few', 'file', 'financial', 'find', 'fine', 'focus', 'for', 'friend', 'from', 'get', 'girlfriend', 'give', 'go', 'going', 'good', 'goodbye', 'great', 'group', 'guess', 'ha', 'had', 'hand', 'happy', 'hate', 'have', 'hay', 'health', 'hello', 'help', 'helpful', 'hey', 'hi', 'highest', 'hmmm', 'hola', 'how', 'howdy', 'i', 'if', 'ill', 'illness', 'importance', 'important', 'in', 'insominia', 'insomnia', 'interested', 'involved', 'is', 'it', 'joke', 'just', 'k', 'kill', 'killing', 'know', 'lack', 'last', 'lately', 'later', 'learn', 'learning', 'leave', 'let', 'life', 'like', 'live', 'location', 'lonely', 'long', 'losing', 'low', 'lowest', 'made', 'maintain', 'make', 'many', 'me', 'mean', 'meaning', 'medication', 'meditation', 'mental', 'mentally', 'mentioned', 'mind', 'mom', 'money', 'more', 'morning', 'much', 'my', 'myself', \"n't\", 'name', 'need', 'never', 'new', 'nice', 'night', 'no', 'nobody', 'not', 'nothing', 'now', 'ntry', 'of', 'oh', 'ok', 'okay', 'on', 'one', 'open', 'option', 'or', 'our', 'out', 'oversleeping', 'parent', 'passed', 'past', 'peak', 'people', 'please', 'possibly', 'practicing', 'prepared', 'prevent', 'probably', 'problem', 'professional', 'proper', 'quit', 'race', 'rate', 'really', 'recover', 'reduce', 'relationship', 'repeating', 'response', 'revoir', 'right', 'robot', 'rule', 'run', 'sad', 'sadness', 'said', 'sasa', 'say', 'saying', 'sayonara', 'scared', 'school', 'see', 'seem', 'sense', 'should', 'shut', 'side', 'sign', 'sister', 'sleep', 'slept', 'so', 'social', 'some', 'someone', 'something', 'sound', 'stage', 'starting', 'stay', 'still', 'stress', 'stressed', 'stuck', 'stupid', 'succeed', 'suffering', 'sugar', 'suicidal', 'suicide', 'support', 'sure', 'symptom', 'take', 'talk', 'tell', 'than', 'thank', 'thanks', 'that', 'the', 'thee', 'them', 'then', 'therapist', 'therapy', 'there', 'think', 'this', 'thought', 'through', 'time', 'to', 'today', 'told', 'top', 'treatment', 'trust', 'type', 'ugly', 'understand', 'understands', 'unwell', 'up', 'useful', 'useless', 'very', 'victim', 'wa', 'want', 'warning', 'way', 'we', 'well', 'were', 'what', 'whatever', 'where', 'which', 'who', 'why', 'will', 'winner', 'wish', 'with', 'world', 'worried', 'worthless', 'would', 'wrong', 'yeah', 'yes', 'you', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "# tokenizing and lematizing\n",
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "data_file = open('intents.json').read()\n",
    "intents = json.loads(data_file)\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "# lemmatize and lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)\n",
    "pickle.dump(words,open('texts.pkl','wb'))\n",
    "pickle.dump(classes,open('labels.pkl','wb'))\n",
    "# create our training data\n",
    "\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = []\n",
    "train_x=[]\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    train_x.append(bag)\n",
    "    train_y.append(output_row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(283, 355)\n"
     ]
    }
   ],
   "source": [
    "train_x=np.array(train_x)\n",
    "train_y=np.array(train_y)\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import zip_longest\n",
    "# import numpy as np\n",
    "# max_length = max(len(seq) for seq in train)\n",
    "# padded_train = [list(zip_longest(*seq, fillvalue=-1)) for seq in train]\n",
    "\n",
    "# # Convert to NumPy array\n",
    "# training = np.array(padded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random \n",
    "# # # shuffle our features and turn into np.array\n",
    "# # random.shuffle(training[:284])\n",
    "# # training = np.array(training)\n",
    "# # create train and test lists. X - patterns, Y - intents\n",
    "# train_x = list(training[:,0])\n",
    "# train_y = list(training[:,1])\n",
    "# print(\"Training data created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "x_train,x_test,y_train,y_test=train_test_split(train_x,train_y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Documents\\python\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "# equal to number of intents to predict output intent with softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(len(train_x[0]), input_shape=(len(train_x[0]),), activation='relu'))\n",
    "# model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">355</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">126,380</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">45,924</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m355\u001b[0m)            │       \u001b[38;5;34m126,380\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m129\u001b[0m)            │        \u001b[38;5;34m45,924\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">172,304</span> (673.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m172,304\u001b[0m (673.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">172,304</span> (673.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m172,304\u001b[0m (673.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Documents\\python\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:34: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9639 - loss: 0.0418 - val_accuracy: 0.9825 - val_loss: 0.0355\n",
      "Epoch 2/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9675 - loss: 0.0460 - val_accuracy: 0.9649 - val_loss: 0.0383\n",
      "Epoch 3/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9904 - loss: 0.0213 - val_accuracy: 0.9649 - val_loss: 0.0456\n",
      "Epoch 4/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9884 - loss: 0.0228 - val_accuracy: 0.9649 - val_loss: 0.0514\n",
      "Epoch 5/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9920 - loss: 0.0180 - val_accuracy: 0.9649 - val_loss: 0.0549\n",
      "Epoch 6/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9882 - loss: 0.0189 - val_accuracy: 0.9649 - val_loss: 0.0627\n",
      "Epoch 7/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9551 - loss: 0.0364 - val_accuracy: 0.9649 - val_loss: 0.0676\n",
      "Epoch 8/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9823 - loss: 0.0153 - val_accuracy: 0.9649 - val_loss: 0.0724\n",
      "Epoch 9/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9880 - loss: 0.0237 - val_accuracy: 0.9649 - val_loss: 0.0795\n",
      "Epoch 10/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9817 - loss: 0.0316 - val_accuracy: 0.9649 - val_loss: 0.0857\n",
      "Epoch 11/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9750 - loss: 0.0330 - val_accuracy: 0.9649 - val_loss: 0.0884\n",
      "Epoch 12/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9876 - loss: 0.0271 - val_accuracy: 0.9649 - val_loss: 0.0961\n",
      "Epoch 13/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9794 - loss: 0.0395 - val_accuracy: 0.9649 - val_loss: 0.1012\n",
      "Epoch 14/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9839 - loss: 0.0264 - val_accuracy: 0.9649 - val_loss: 0.1064\n",
      "Epoch 15/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9844 - loss: 0.0217 - val_accuracy: 0.9474 - val_loss: 0.1106\n",
      "Epoch 16/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9808 - loss: 0.0204 - val_accuracy: 0.9474 - val_loss: 0.1159\n",
      "Epoch 17/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9772 - loss: 0.0210 - val_accuracy: 0.9649 - val_loss: 0.1184\n",
      "Epoch 18/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9595 - loss: 0.0316 - val_accuracy: 0.9649 - val_loss: 0.1238\n",
      "Epoch 19/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9967 - loss: 0.0155 - val_accuracy: 0.9649 - val_loss: 0.1253\n",
      "Epoch 20/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9734 - loss: 0.0301 - val_accuracy: 0.9649 - val_loss: 0.1301\n",
      "Epoch 21/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9892 - loss: 0.0307 - val_accuracy: 0.9474 - val_loss: 0.1355\n",
      "Epoch 22/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9827 - loss: 0.0157 - val_accuracy: 0.9649 - val_loss: 0.1378\n",
      "Epoch 23/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9731 - loss: 0.0257 - val_accuracy: 0.9649 - val_loss: 0.1397\n",
      "Epoch 24/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9928 - loss: 0.0206 - val_accuracy: 0.9649 - val_loss: 0.1446\n",
      "Epoch 25/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9760 - loss: 0.0263 - val_accuracy: 0.9649 - val_loss: 0.1479\n",
      "Epoch 26/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9727 - loss: 0.0319 - val_accuracy: 0.9649 - val_loss: 0.1521\n",
      "Epoch 27/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9728 - loss: 0.0342 - val_accuracy: 0.9649 - val_loss: 0.1549\n",
      "Epoch 28/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9780 - loss: 0.0286 - val_accuracy: 0.9474 - val_loss: 0.1589\n",
      "Epoch 29/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9876 - loss: 0.0202 - val_accuracy: 0.9474 - val_loss: 0.1605\n",
      "Epoch 30/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9664 - loss: 0.0344 - val_accuracy: 0.9649 - val_loss: 0.1628\n",
      "Epoch 31/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9607 - loss: 0.0335 - val_accuracy: 0.9474 - val_loss: 0.1670\n",
      "Epoch 32/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9872 - loss: 0.0220 - val_accuracy: 0.9649 - val_loss: 0.1687\n",
      "Epoch 33/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9715 - loss: 0.0349 - val_accuracy: 0.9649 - val_loss: 0.1713\n",
      "Epoch 34/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9646 - loss: 0.0385 - val_accuracy: 0.9474 - val_loss: 0.1749\n",
      "Epoch 35/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9834 - loss: 0.0183 - val_accuracy: 0.9474 - val_loss: 0.1777\n",
      "Epoch 36/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9727 - loss: 0.0318 - val_accuracy: 0.9649 - val_loss: 0.1789\n",
      "Epoch 37/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9788 - loss: 0.0197 - val_accuracy: 0.9649 - val_loss: 0.1806\n",
      "Epoch 38/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9727 - loss: 0.0369 - val_accuracy: 0.9649 - val_loss: 0.1841\n",
      "Epoch 39/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9698 - loss: 0.0240 - val_accuracy: 0.9474 - val_loss: 0.1864\n",
      "Epoch 40/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9640 - loss: 0.0299 - val_accuracy: 0.9649 - val_loss: 0.1869\n",
      "Epoch 41/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9928 - loss: 0.0157 - val_accuracy: 0.9474 - val_loss: 0.1923\n",
      "Epoch 42/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9884 - loss: 0.0173 - val_accuracy: 0.9474 - val_loss: 0.1933\n",
      "Epoch 43/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9765 - loss: 0.0279 - val_accuracy: 0.9474 - val_loss: 0.1951\n",
      "Epoch 44/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9718 - loss: 0.0335 - val_accuracy: 0.9474 - val_loss: 0.1967\n",
      "Epoch 45/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9903 - loss: 0.0279 - val_accuracy: 0.9474 - val_loss: 0.1989\n",
      "Epoch 46/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9892 - loss: 0.0167 - val_accuracy: 0.9649 - val_loss: 0.1983\n",
      "Epoch 47/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9657 - loss: 0.0332 - val_accuracy: 0.9474 - val_loss: 0.2002\n",
      "Epoch 48/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9953 - loss: 0.0106 - val_accuracy: 0.9474 - val_loss: 0.2035\n",
      "Epoch 49/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9829 - loss: 0.0186 - val_accuracy: 0.9649 - val_loss: 0.2033\n",
      "Epoch 50/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9915 - loss: 0.0144 - val_accuracy: 0.9649 - val_loss: 0.2046\n",
      "Epoch 51/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9843 - loss: 0.0222 - val_accuracy: 0.9649 - val_loss: 0.2066\n",
      "Epoch 52/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9844 - loss: 0.0301 - val_accuracy: 0.9649 - val_loss: 0.2083\n",
      "Epoch 53/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9907 - loss: 0.0204 - val_accuracy: 0.9649 - val_loss: 0.2093\n",
      "Epoch 54/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9876 - loss: 0.0227 - val_accuracy: 0.9649 - val_loss: 0.2098\n",
      "Epoch 55/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9876 - loss: 0.0235 - val_accuracy: 0.9474 - val_loss: 0.2149\n",
      "Epoch 56/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9880 - loss: 0.0200 - val_accuracy: 0.9474 - val_loss: 0.2183\n",
      "Epoch 57/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9872 - loss: 0.0246 - val_accuracy: 0.9474 - val_loss: 0.2179\n",
      "Epoch 58/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9776 - loss: 0.0223 - val_accuracy: 0.9649 - val_loss: 0.2178\n",
      "Epoch 59/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9876 - loss: 0.0166 - val_accuracy: 0.9474 - val_loss: 0.2199\n",
      "Epoch 60/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9807 - loss: 0.0160 - val_accuracy: 0.9649 - val_loss: 0.2192\n",
      "Epoch 61/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9681 - loss: 0.0354 - val_accuracy: 0.9649 - val_loss: 0.2209\n",
      "Epoch 62/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9929 - loss: 0.0170 - val_accuracy: 0.9474 - val_loss: 0.2225\n",
      "Epoch 63/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9930 - loss: 0.0111 - val_accuracy: 0.9474 - val_loss: 0.2239\n",
      "Epoch 64/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9695 - loss: 0.0370 - val_accuracy: 0.9649 - val_loss: 0.2239\n",
      "Epoch 65/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9861 - loss: 0.0129 - val_accuracy: 0.9649 - val_loss: 0.2254\n",
      "Epoch 66/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9887 - loss: 0.0123 - val_accuracy: 0.9649 - val_loss: 0.2247\n",
      "Epoch 67/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9673 - loss: 0.0328 - val_accuracy: 0.9474 - val_loss: 0.2291\n",
      "Epoch 68/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9842 - loss: 0.0208 - val_accuracy: 0.9474 - val_loss: 0.2298\n",
      "Epoch 69/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9826 - loss: 0.0264 - val_accuracy: 0.9474 - val_loss: 0.2318\n",
      "Epoch 70/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9744 - loss: 0.0257 - val_accuracy: 0.9474 - val_loss: 0.2314\n",
      "Epoch 71/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9837 - loss: 0.0226 - val_accuracy: 0.9474 - val_loss: 0.2321\n",
      "Epoch 72/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9808 - loss: 0.0225 - val_accuracy: 0.9474 - val_loss: 0.2342\n",
      "Epoch 73/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9817 - loss: 0.0359 - val_accuracy: 0.9474 - val_loss: 0.2371\n",
      "Epoch 74/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9701 - loss: 0.0227 - val_accuracy: 0.9474 - val_loss: 0.2348\n",
      "Epoch 75/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9784 - loss: 0.0184 - val_accuracy: 0.9474 - val_loss: 0.2377\n",
      "Epoch 76/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9816 - loss: 0.0188 - val_accuracy: 0.9474 - val_loss: 0.2376\n",
      "Epoch 77/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9858 - loss: 0.0272 - val_accuracy: 0.9474 - val_loss: 0.2394\n",
      "Epoch 78/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9863 - loss: 0.0194 - val_accuracy: 0.9474 - val_loss: 0.2398\n",
      "Epoch 79/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9866 - loss: 0.0157 - val_accuracy: 0.9649 - val_loss: 0.2406\n",
      "Epoch 80/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9571 - loss: 0.0385 - val_accuracy: 0.9474 - val_loss: 0.2418\n",
      "Epoch 81/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9798 - loss: 0.0246 - val_accuracy: 0.9649 - val_loss: 0.2422\n",
      "Epoch 82/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9719 - loss: 0.0227 - val_accuracy: 0.9474 - val_loss: 0.2438\n",
      "Epoch 83/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9791 - loss: 0.0269 - val_accuracy: 0.9474 - val_loss: 0.2444\n",
      "Epoch 84/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9756 - loss: 0.0248 - val_accuracy: 0.9474 - val_loss: 0.2467\n",
      "Epoch 85/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9868 - loss: 0.0138 - val_accuracy: 0.9649 - val_loss: 0.2460\n",
      "Epoch 86/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9794 - loss: 0.0219 - val_accuracy: 0.9649 - val_loss: 0.2470\n",
      "Epoch 87/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9688 - loss: 0.0282 - val_accuracy: 0.9474 - val_loss: 0.2488\n",
      "Epoch 88/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9857 - loss: 0.0186 - val_accuracy: 0.9649 - val_loss: 0.2489\n",
      "Epoch 89/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9779 - loss: 0.0217 - val_accuracy: 0.9649 - val_loss: 0.2488\n",
      "Epoch 90/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9670 - loss: 0.0322 - val_accuracy: 0.9649 - val_loss: 0.2505\n",
      "Epoch 91/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9793 - loss: 0.0230 - val_accuracy: 0.9474 - val_loss: 0.2531\n",
      "Epoch 92/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9879 - loss: 0.0199 - val_accuracy: 0.9649 - val_loss: 0.2524\n",
      "Epoch 93/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9804 - loss: 0.0304 - val_accuracy: 0.9649 - val_loss: 0.2521\n",
      "Epoch 94/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9905 - loss: 0.0177 - val_accuracy: 0.9474 - val_loss: 0.2561\n",
      "Epoch 95/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9831 - loss: 0.0272 - val_accuracy: 0.9474 - val_loss: 0.2572\n",
      "Epoch 96/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9594 - loss: 0.0326 - val_accuracy: 0.9474 - val_loss: 0.2575\n",
      "Epoch 97/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9823 - loss: 0.0223 - val_accuracy: 0.9474 - val_loss: 0.2592\n",
      "Epoch 98/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9912 - loss: 0.0170 - val_accuracy: 0.9474 - val_loss: 0.2591\n",
      "Epoch 99/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9908 - loss: 0.0140 - val_accuracy: 0.9474 - val_loss: 0.2622\n",
      "Epoch 100/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9673 - loss: 0.0337 - val_accuracy: 0.9474 - val_loss: 0.2621\n",
      "Epoch 101/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9801 - loss: 0.0216 - val_accuracy: 0.9474 - val_loss: 0.2626\n",
      "Epoch 102/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9783 - loss: 0.0277 - val_accuracy: 0.9474 - val_loss: 0.2635\n",
      "Epoch 103/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9917 - loss: 0.0263 - val_accuracy: 0.9474 - val_loss: 0.2647\n",
      "Epoch 104/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9895 - loss: 0.0150 - val_accuracy: 0.9474 - val_loss: 0.2655\n",
      "Epoch 105/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9863 - loss: 0.0402 - val_accuracy: 0.9474 - val_loss: 0.2673\n",
      "Epoch 106/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9734 - loss: 0.0225 - val_accuracy: 0.9649 - val_loss: 0.2667\n",
      "Epoch 107/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9698 - loss: 0.0398 - val_accuracy: 0.9474 - val_loss: 0.2687\n",
      "Epoch 108/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9820 - loss: 0.0218 - val_accuracy: 0.9474 - val_loss: 0.2691\n",
      "Epoch 109/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9822 - loss: 0.0228 - val_accuracy: 0.9474 - val_loss: 0.2707\n",
      "Epoch 110/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9812 - loss: 0.0274 - val_accuracy: 0.9649 - val_loss: 0.2703\n",
      "Epoch 111/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9674 - loss: 0.0332 - val_accuracy: 0.9474 - val_loss: 0.2719\n",
      "Epoch 112/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9919 - loss: 0.0255 - val_accuracy: 0.9474 - val_loss: 0.2738\n",
      "Epoch 113/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9762 - loss: 0.0202 - val_accuracy: 0.9649 - val_loss: 0.2724\n",
      "Epoch 114/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9601 - loss: 0.0417 - val_accuracy: 0.9474 - val_loss: 0.2746\n",
      "Epoch 115/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9821 - loss: 0.0344 - val_accuracy: 0.9649 - val_loss: 0.2746\n",
      "Epoch 116/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9743 - loss: 0.0249 - val_accuracy: 0.9474 - val_loss: 0.2774\n",
      "Epoch 117/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9655 - loss: 0.0297 - val_accuracy: 0.9649 - val_loss: 0.2770\n",
      "Epoch 118/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9719 - loss: 0.0223 - val_accuracy: 0.9649 - val_loss: 0.2774\n",
      "Epoch 119/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9711 - loss: 0.0235 - val_accuracy: 0.9474 - val_loss: 0.2815\n",
      "Epoch 120/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9818 - loss: 0.0416 - val_accuracy: 0.9474 - val_loss: 0.2816\n",
      "Epoch 121/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9811 - loss: 0.0281 - val_accuracy: 0.9474 - val_loss: 0.2822\n",
      "Epoch 122/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9618 - loss: 0.0316 - val_accuracy: 0.9649 - val_loss: 0.2819\n",
      "Epoch 123/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9834 - loss: 0.0204 - val_accuracy: 0.9474 - val_loss: 0.2836\n",
      "Epoch 124/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9831 - loss: 0.0212 - val_accuracy: 0.9474 - val_loss: 0.2861\n",
      "Epoch 125/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9636 - loss: 0.0276 - val_accuracy: 0.9649 - val_loss: 0.2848\n",
      "Epoch 126/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9734 - loss: 0.0254 - val_accuracy: 0.9474 - val_loss: 0.2871\n",
      "Epoch 127/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9691 - loss: 0.0277 - val_accuracy: 0.9649 - val_loss: 0.2876\n",
      "Epoch 128/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9766 - loss: 0.0229 - val_accuracy: 0.9474 - val_loss: 0.2893\n",
      "Epoch 129/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9755 - loss: 0.0255 - val_accuracy: 0.9474 - val_loss: 0.2896\n",
      "Epoch 130/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9758 - loss: 0.0263 - val_accuracy: 0.9474 - val_loss: 0.2907\n",
      "Epoch 131/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9646 - loss: 0.0306 - val_accuracy: 0.9649 - val_loss: 0.2910\n",
      "Epoch 132/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9942 - loss: 0.0099 - val_accuracy: 0.9649 - val_loss: 0.2916\n",
      "Epoch 133/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9847 - loss: 0.0226 - val_accuracy: 0.9474 - val_loss: 0.2935\n",
      "Epoch 134/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9798 - loss: 0.0199 - val_accuracy: 0.9649 - val_loss: 0.2930\n",
      "Epoch 135/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9923 - loss: 0.0289 - val_accuracy: 0.9649 - val_loss: 0.2939\n",
      "Epoch 136/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9883 - loss: 0.0185 - val_accuracy: 0.9474 - val_loss: 0.2956\n",
      "Epoch 137/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9802 - loss: 0.0202 - val_accuracy: 0.9474 - val_loss: 0.2963\n",
      "Epoch 138/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9710 - loss: 0.0230 - val_accuracy: 0.9649 - val_loss: 0.2964\n",
      "Epoch 139/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9825 - loss: 0.0255 - val_accuracy: 0.9649 - val_loss: 0.2971\n",
      "Epoch 140/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9858 - loss: 0.0232 - val_accuracy: 0.9474 - val_loss: 0.2988\n",
      "Epoch 141/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9784 - loss: 0.0299 - val_accuracy: 0.9474 - val_loss: 0.2999\n",
      "Epoch 142/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9848 - loss: 0.0284 - val_accuracy: 0.9474 - val_loss: 0.3010\n",
      "Epoch 143/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9792 - loss: 0.0285 - val_accuracy: 0.9474 - val_loss: 0.3022\n",
      "Epoch 144/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9678 - loss: 0.0304 - val_accuracy: 0.9649 - val_loss: 0.3015\n",
      "Epoch 145/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9806 - loss: 0.0245 - val_accuracy: 0.9649 - val_loss: 0.3019\n",
      "Epoch 146/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9889 - loss: 0.0225 - val_accuracy: 0.9649 - val_loss: 0.3031\n",
      "Epoch 147/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9834 - loss: 0.0168 - val_accuracy: 0.9474 - val_loss: 0.3048\n",
      "Epoch 148/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9758 - loss: 0.0259 - val_accuracy: 0.9474 - val_loss: 0.3055\n",
      "Epoch 149/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9722 - loss: 0.0259 - val_accuracy: 0.9649 - val_loss: 0.3055\n",
      "Epoch 150/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9889 - loss: 0.0209 - val_accuracy: 0.9474 - val_loss: 0.3074\n",
      "Epoch 151/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9667 - loss: 0.0268 - val_accuracy: 0.9649 - val_loss: 0.3074\n",
      "Epoch 152/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9885 - loss: 0.0199 - val_accuracy: 0.9649 - val_loss: 0.3078\n",
      "Epoch 153/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9829 - loss: 0.0212 - val_accuracy: 0.9649 - val_loss: 0.3090\n",
      "Epoch 154/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9766 - loss: 0.0229 - val_accuracy: 0.9474 - val_loss: 0.3103\n",
      "Epoch 155/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9636 - loss: 0.0356 - val_accuracy: 0.9474 - val_loss: 0.3113\n",
      "Epoch 156/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9923 - loss: 0.0123 - val_accuracy: 0.9474 - val_loss: 0.3129\n",
      "Epoch 157/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9772 - loss: 0.0225 - val_accuracy: 0.9474 - val_loss: 0.3144\n",
      "Epoch 158/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9682 - loss: 0.0303 - val_accuracy: 0.9649 - val_loss: 0.3133\n",
      "Epoch 159/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9919 - loss: 0.0204 - val_accuracy: 0.9649 - val_loss: 0.3138\n",
      "Epoch 160/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9835 - loss: 0.0270 - val_accuracy: 0.9649 - val_loss: 0.3149\n",
      "Epoch 161/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9815 - loss: 0.0268 - val_accuracy: 0.9474 - val_loss: 0.3165\n",
      "Epoch 162/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9772 - loss: 0.0234 - val_accuracy: 0.9649 - val_loss: 0.3171\n",
      "Epoch 163/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9848 - loss: 0.0170 - val_accuracy: 0.9474 - val_loss: 0.3182\n",
      "Epoch 164/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9515 - loss: 0.0349 - val_accuracy: 0.9474 - val_loss: 0.3187\n",
      "Epoch 165/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9685 - loss: 0.0270 - val_accuracy: 0.9474 - val_loss: 0.3196\n",
      "Epoch 166/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9900 - loss: 0.0251 - val_accuracy: 0.9474 - val_loss: 0.3205\n",
      "Epoch 167/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9748 - loss: 0.0274 - val_accuracy: 0.9649 - val_loss: 0.3203\n",
      "Epoch 168/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9518 - loss: 0.0386 - val_accuracy: 0.9474 - val_loss: 0.3224\n",
      "Epoch 169/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9811 - loss: 0.0293 - val_accuracy: 0.9649 - val_loss: 0.3218\n",
      "Epoch 170/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9931 - loss: 0.0222 - val_accuracy: 0.9474 - val_loss: 0.3251\n",
      "Epoch 171/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9772 - loss: 0.0242 - val_accuracy: 0.9474 - val_loss: 0.3259\n",
      "Epoch 172/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9745 - loss: 0.0288 - val_accuracy: 0.9474 - val_loss: 0.3254\n",
      "Epoch 173/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9825 - loss: 0.0173 - val_accuracy: 0.9474 - val_loss: 0.3268\n",
      "Epoch 174/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9743 - loss: 0.0262 - val_accuracy: 0.9474 - val_loss: 0.3275\n",
      "Epoch 175/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9808 - loss: 0.0312 - val_accuracy: 0.9474 - val_loss: 0.3290\n",
      "Epoch 176/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9635 - loss: 0.0299 - val_accuracy: 0.9474 - val_loss: 0.3287\n",
      "Epoch 177/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9763 - loss: 0.0288 - val_accuracy: 0.9649 - val_loss: 0.3292\n",
      "Epoch 178/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9522 - loss: 0.0348 - val_accuracy: 0.9649 - val_loss: 0.3296\n",
      "Epoch 179/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9784 - loss: 0.0330 - val_accuracy: 0.9474 - val_loss: 0.3315\n",
      "Epoch 180/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9788 - loss: 0.0233 - val_accuracy: 0.9474 - val_loss: 0.3337\n",
      "Epoch 181/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9794 - loss: 0.0343 - val_accuracy: 0.9649 - val_loss: 0.3317\n",
      "Epoch 182/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9806 - loss: 0.0169 - val_accuracy: 0.9474 - val_loss: 0.3340\n",
      "Epoch 183/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9908 - loss: 0.0330 - val_accuracy: 0.9474 - val_loss: 0.3350\n",
      "Epoch 184/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9849 - loss: 0.0325 - val_accuracy: 0.9474 - val_loss: 0.3351\n",
      "Epoch 185/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9775 - loss: 0.0223 - val_accuracy: 0.9649 - val_loss: 0.3351\n",
      "Epoch 186/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9809 - loss: 0.0221 - val_accuracy: 0.9649 - val_loss: 0.3354\n",
      "Epoch 187/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9883 - loss: 0.0193 - val_accuracy: 0.9474 - val_loss: 0.3380\n",
      "Epoch 188/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9722 - loss: 0.0328 - val_accuracy: 0.9649 - val_loss: 0.3377\n",
      "Epoch 189/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9903 - loss: 0.0122 - val_accuracy: 0.9649 - val_loss: 0.3379\n",
      "Epoch 190/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9775 - loss: 0.0231 - val_accuracy: 0.9649 - val_loss: 0.3395\n",
      "Epoch 191/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9932 - loss: 0.0246 - val_accuracy: 0.9649 - val_loss: 0.3395\n",
      "Epoch 192/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9674 - loss: 0.0265 - val_accuracy: 0.9649 - val_loss: 0.3409\n",
      "Epoch 193/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9894 - loss: 0.0200 - val_accuracy: 0.9649 - val_loss: 0.3413\n",
      "Epoch 194/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9815 - loss: 0.0253 - val_accuracy: 0.9649 - val_loss: 0.3427\n",
      "Epoch 195/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9905 - loss: 0.0117 - val_accuracy: 0.9474 - val_loss: 0.3436\n",
      "Epoch 196/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9884 - loss: 0.0314 - val_accuracy: 0.9474 - val_loss: 0.3450\n",
      "Epoch 197/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9657 - loss: 0.0274 - val_accuracy: 0.9649 - val_loss: 0.3447\n",
      "Epoch 198/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9920 - loss: 0.0173 - val_accuracy: 0.9649 - val_loss: 0.3449\n",
      "Epoch 199/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9808 - loss: 0.0263 - val_accuracy: 0.9649 - val_loss: 0.3461\n",
      "Epoch 200/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9748 - loss: 0.0412 - val_accuracy: 0.9474 - val_loss: 0.3482\n"
     ]
    }
   ],
   "source": [
    "#fitting and saving the model \n",
    "hist = model.fit(x_train,y_train , epochs=200, batch_size=10,validation_data=(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - accuracy: 0.9441 - loss: 0.3299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34820958971977234, 0.9473684430122375]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# Save model architecture to JSON file\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save model weights to HDF5 file\n",
    "model.save_weights(\"model_weights.weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(train_x)[104])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5c2a1cb3f7f6160aa571082c3edf6897403ba248133963500dc500635300624"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
